{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Experiment 4_Voice Cloning_Beta-Testing.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "uCw5GYvVf9Ul"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_nnbUFp5L5d"
      },
      "source": [
        "# ðŸ§ª Experiment 4: Voice Cloning ðŸŽ¤\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-FJX7PGmEHH"
      },
      "source": [
        "Welcome back, all, for our fourth weekly homework! This time, we're all about sound ðŸŽµ\n",
        "\n",
        "Deepfakes are not only about images and video, but also audio! In this assignment you will learn some techniques for deepfaking voices. We will walkthrough how we can use pre-trained models that are offered freely online for creating deep-faked voices.  \n",
        "\n",
        "We will cover two methods. One is \"Voice-Cloning\" based on a neural network-based system for text-to-speech synthesis (TTS). Another is expressive voice synthesis, where you can make a voice emote and sing (rhythm and pitch). \n",
        "\n",
        "**Setup**: Make sure your GPU is enabled as the runtime in Colab (Runtime -> Change Runtime Type -> Hardware Accelerator -> GPU)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qznkjWnK5L5f"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\" style=\"text-align: center;\">\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/mitmedialab/MAS.S60.Fall2020/blob/master/homework/HW3_Voice_Cloning.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" /> Run in Google Colab</a>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GekPfwlh1jzn"
      },
      "source": [
        "# 1) Voice Cloning\n",
        "\n",
        "This is based off of a work called: **Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Aynthesis (SV2TTS)**\n",
        "\n",
        "SV2TTS is a three-stage deep learning framework that allows to create a numerical representation of a voice from a few seconds of audio, and to use it to condition a text-to-speech model trained to generalize to new voices.\n",
        "\n",
        "It comprises:\n",
        "1.   Speaker encoder (computes a fixed dimensional vector from a speech signal)\n",
        "2.   Synthesizer (predicts a mel spectrogram from phonemes & speech input)\n",
        "3.   Vocoder (converts the spectrogram into time domain waveforms)\n",
        "\n",
        "\n",
        "**Resources**\n",
        "*   Original project: https://github.com/CorentinJ/Real-Time-Voice-Cloning \n",
        "*   Paper: https://arxiv.org/pdf/1806.04558.pdf\n",
        "*   Modfied project (We will use this one): https://github.com/contractorwolf/Real-Time-Voice-Cloning \n",
        "* Blog Post: https://medium.com/analytics-vidhya/the-intuition-behind-voice-cloning-with-5-seconds-of-audio-5989e9b2e042\n",
        "*  2 minute paper summary: https://www.youtube.com/watch?v=0sR1rU3gLzQ&ab_channel=TwoMinutePapers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8l6Dfdp1rED"
      },
      "source": [
        "**Summary**:In the following process you will install the audio libraries, download the repo, install the requirments for the repo, download the pretrained voice model, record a sample WAV file of your voice, input the new text, apply the voice sample to the pretrained model and create an output WAV file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMBgTSMR1xdd"
      },
      "source": [
        "**Step 1**. Install the required audio libraries, download the modified project repo and install the repository's requirements."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "um9c_i551lk2"
      },
      "source": [
        "!apt-get install libportaudio2\n",
        "print(\"\\nINSTALL COMPLETE: libportaudio2\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pG2xoCf93bmd"
      },
      "source": [
        "!git clone https://github.com/contractorwolf/Real-Time-Voice-Cloning.git  \n",
        "print(\"\\nDOWNLOAD COMPLETE: Real-Time-Voice-Cloning repo\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fEem4Ot3dr5"
      },
      "source": [
        "cd Real-Time-Voice-Cloning/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHeA4HdA3pX2"
      },
      "source": [
        "!pip install -r requirements.txt\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "!pip uninstall -y tensorflow\n",
        "!pip install tensorflow-gpu==1.15.0\n",
        "\n",
        "print(\"\\nREQUIREMENTS INSTALLED\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjPMa_a235ZN"
      },
      "source": [
        "**Step 2.** Download and unzip the generic pretrained voice model that used audiobooks (text and voice) as the source. This source lacks tone, but is useful as a pretrained model. We'll fine tune this pretrained model to make a clone of your voice. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjcruFiu41pO"
      },
      "source": [
        "# !gdown https://drive.google.com/uc?id=1n1sPXvT34yXFLT47QZA6FIRGrwMeSsZc #< original model hosted here\n",
        "!gdown https://drive.google.com/uc?id=16usTDw-3kVAWd6Xdzz-_7Tc1Cb4ZWUws #< Joanne's copy of the pretrained model on google drive, with 'anyone with link' permission\n",
        "\n",
        "print(\"UNZIPPING: pretrained.zip\")\n",
        "!unzip pretrained.zip\n",
        "print(\"UNZIP COMPLETE\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbK5ApBn6QW4"
      },
      "source": [
        "**Step 3.** Load the libaries and fancy javascript used for using the browser to do the sample audio recording. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkbk0wWv6Ub8"
      },
      "source": [
        "# all imports\n",
        "from IPython.display import Javascript\n",
        "from google.colab import output\n",
        "from base64 import b64decode\n",
        "import IPython.display as ipd\n",
        "\n",
        "RECORD = \"\"\"\n",
        "const sleep  = time => new Promise(resolve => setTimeout(resolve, time))\n",
        "const b2text = blob => new Promise(resolve => {\n",
        "  const reader = new FileReader()\n",
        "  reader.onloadend = e => resolve(e.srcElement.result)\n",
        "  reader.readAsDataURL(blob)\n",
        "})\n",
        "var record = time => new Promise(async resolve => {\n",
        "  stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
        "  recorder = new MediaRecorder(stream)\n",
        "  chunks = []\n",
        "  recorder.ondataavailable = e => chunks.push(e.data)\n",
        "  recorder.start()\n",
        "  await sleep(time)\n",
        "  recorder.onstop = async ()=>{\n",
        "    blob = new Blob(chunks)\n",
        "    text = await b2text(blob)\n",
        "    resolve(text)\n",
        "  }\n",
        "  recorder.stop()\n",
        "})\n",
        "\"\"\"\n",
        "\n",
        "def record(filename, sec=3):\n",
        "  display(Javascript(RECORD))\n",
        "  s = output.eval_js('record(%d)' % (sec*1000))\n",
        "  b = b64decode(s.split(',')[1])\n",
        "  with open(filename,'wb') as f:\n",
        "    f.write(b)\n",
        "  return 'FILE RECORDED: ' + filename\n",
        "\n",
        "\n",
        "print(\"libraries and javascript loaded\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zU4JO4Ea5H-R"
      },
      "source": [
        "**Step 4**. Record your own voice sample! This will be used clone your voice. Note that Once you play the next code block, it will immediately start recording. Only stop when the block of code has completed (approx. 10s). \n",
        "\n",
        "**READ THIS ALOUD:**\n",
        "\n",
        "The Pro is an upgraded version of the Classic. In the pro model you get an electrical system, a super powerful rear hub brushless motor with tons of torque, an easy-access battery with port for phone charging, and Fat Tires with puncture-resistant casings.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZZ31nV15G8r"
      },
      "source": [
        "record('input.wav', sec=10)\n",
        "print(\"RECORDING COMPLETE\")\n",
        "ipd.Audio('input.wav') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UI0cYqb5PIW"
      },
      "source": [
        "**Step 5.** Executing the python command below will create a voice signature from the previously recorded audio sample (using the downloaded translate.py). \n",
        "\n",
        "Your cloned voice will say what you pass into the **textin** parameter.  to generate an audio file named output.wav\n",
        "\n",
        "The output will be located here (in the filebrowser of this colab notebook): /content/Real-Time-Voice-Cloning/output.wav"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "CzmNghnLbCKQ"
      },
      "source": [
        "#@title code patch\n",
        "# 2021-10-11\n",
        "# update translate.py file to use soundfile.write instead of librosa.output\n",
        "\n",
        "with open(\"translate.py\",\"r\") as f_read:\n",
        "  code = f_read.read()\n",
        "  code = code.replace(\"import librosa\",\"import soundfile as sf\")\n",
        "  code = code.replace(\"librosa.output.write_wav(fpath, generated_wav.astype(np.float32), synthesizer.sample_rate)\",\"sf.write(fpath, generated_wav.astype(np.float32), synthesizer.sample_rate)\")\n",
        "  #print(code)\n",
        "\n",
        "with open(\"translate.py\",\"w\") as f_write:\n",
        "  f_write.write(code)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3x6wuq65dBS"
      },
      "source": [
        "# THIS STEP  WILL PRODUCE A LOT OF TEXT OUTPUT (warnings), it will show a WAV file when complete\n",
        "# the textin parameter used below should be a text sentence about 20 words long, less leaves weird spaces in output file\n",
        "\n",
        "!python translate.py \\\n",
        "  --textin=\"This is a project that started as a complex thesis for pattern voice matching using Googles Tensorflow\"\n",
        "\n",
        "  # other file params that can be passed\n",
        "  #--voicein=\"input.mp3\" #the translate.py file has a default value of: \"input.wav\", only change it if needed\n",
        "\n",
        "print(\"TRANSLATION COMPLETE\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sin3N4-O5ce3"
      },
      "source": [
        "**Play output file which should be the incoming_text modeled with the voice signature file.** Not bad, given the short recording of your voice. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8olFvBdz52ul"
      },
      "source": [
        "print(\"Cloned voice saying input text\")\n",
        "ipd.Audio('output.wav') # load a local WAV file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tKbHwui51zc"
      },
      "source": [
        "**You can compare it to original voice sample recording.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9tCNCOG6DqE"
      },
      "source": [
        "print(\"Sample voice recording\")\n",
        "ipd.Audio('input.wav') # load a local WAV file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCw5GYvVf9Ul"
      },
      "source": [
        "# 2) Mellotron\n",
        "\n",
        "**Mellotron** is a multispeaker voice synthesis model based on Tacotron 2 GST that can make a voice emote and sing without emotive or singing training data. By explicitly conditioning on rhythm and continuous pitch contours from an audio signal or music score, Mellotron is able to generate speech in a variety of styles ranging from read speech to expressive speech, from slow drawls to rap and from monotonous voice to singing voice.\n",
        "\n",
        "**Resources**\n",
        "\n",
        "* Source: https://github.com/NVIDIA/mellotron\n",
        "\n",
        "* Demo: https://nv-adlr.github.io/Mellotron\n",
        "\n",
        "* Paper: https://arxiv.org/pdf/1910.11997.pdf\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This section of the notebook is based on a demo for Mellotron \n",
        "by [Hyungon Ryu](https://github.com/yhgon)  | NVAITC Sr. Data Scientist | Center Lead @ NVIDIA \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7z4NBPmKbKH"
      },
      "source": [
        "**Step 1.** Install required python modules and APEX, and download the official checkpoint. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HP_GTxJ1gASN"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "co-XgHGwKmXn"
      },
      "source": [
        "%%time\n",
        "%%bash\n",
        "\n",
        "pip install  tensorflow==1.15 inflect==0.2.5 librosa==0.6.0  tensorboardX==1.1 Unidecode==1.0.22 pillow nltk==3.4.5 jamo==0.4.1  music21 vamp > /dev/null \n",
        "pip install  tensorflow==1.15 inflect==0.2.5 librosa==0.6.0  tensorboardX==1.1 Unidecode==1.0.22 pillow nltk==3.4.5 jamo==0.4.1  scipy==1.4.1  gast==0.2.2 matplotlib==3.1.1 imgaug==0.2.7\n",
        "\n",
        "pip install --upgrade scikit-image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rn4fDeACKylV"
      },
      "source": [
        "%%time\n",
        "%%bash\n",
        "git clone https://github.com/NVIDIA/apex\n",
        "cd apex\n",
        "pip install  --no-cache-dir ./ # only python  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyDd-oPDKz1q"
      },
      "source": [
        "%%bash\n",
        "git clone https://github.com/NVIDIA/mellotron.git /content/mellotron\n",
        "cd /content/mellotron\n",
        "git submodule init\n",
        "git submodule update"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVQYA2JAK0uS"
      },
      "source": [
        "## Download Official Checkpoint. Use google drive utilities.\n",
        "\n",
        "%%bash\n",
        "wget -N  -q https://raw.githubusercontent.com/yhgon/colab_utils/master/gfile.py\n",
        "python gfile.py -u 'https://drive.google.com/open?id=1ZesPPyRRKloltRIuRnGZ2LIUEuMSVjkI' -f '/content/mellotron_libritts.pt'\n",
        "\n",
        "#Let's use this pre-trained model from: https://github.com/NVIDIA/waveglow \n",
        "python gfile.py -u 'https://drive.google.com/open?id=1rpK8CzAAirq9sWZhe9nlfvxMF1dRgFbF' -f '/content/waveglow_256channels_universal_v5.pt' "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQjp2peoK_gx"
      },
      "source": [
        "**Step 2.** Run patches.\n",
        "\n",
        "It's a temporal solution for inference on COLAB\n",
        "- modify CMUDict directories in `hparams.py` with `cmudict_path=\"/content/mellotron/data/cmu_dictionary\"`\n",
        "- ignore distributed module using `train_utils.py` instead of `train.py`\n",
        "- modify CMUDict directories for `CMUDICT_PATH`  `/content/mellotron/data/cmu_dictionary`  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "fVb0sElrLGr_"
      },
      "source": [
        "#@title\n",
        "%%file hparams.py\n",
        "import tensorflow as tf\n",
        "from text.symbols import symbols\n",
        "\n",
        "\n",
        "def create_hparams(hparams_string=None, verbose=False):\n",
        "    \"\"\"Create model hyperparameters. Parse nondefault from given string.\"\"\"\n",
        "\n",
        "    hparams = tf.contrib.training.HParams(\n",
        "        ################################\n",
        "        # Experiment Parameters        #\n",
        "        ################################\n",
        "        epochs=50000,\n",
        "        iters_per_checkpoint=500,\n",
        "        seed=1234,\n",
        "        dynamic_loss_scaling=True,\n",
        "        fp16_run=False,\n",
        "        distributed_run=False,\n",
        "        dist_backend=\"nccl\",\n",
        "        dist_url=\"tcp://localhost:54321\",\n",
        "        cudnn_enabled=True,\n",
        "        cudnn_benchmark=False,\n",
        "        ignore_layers=['speaker_embedding.weight'],\n",
        "\n",
        "        ################################\n",
        "        # Data Parameters             #\n",
        "        ################################\n",
        "        training_files='filelists/ljs_audiopaths_text_sid_train_filelist.txt',\n",
        "        validation_files='filelists/ljs_audiopaths_text_sid_val_filelist.txt',\n",
        "        text_cleaners=['english_cleaners'],\n",
        "        p_arpabet=1.0,\n",
        "        cmudict_path=\"/content/mellotron/data/cmu_dictionary\",\n",
        "\n",
        "        ################################\n",
        "        # Audio Parameters             #\n",
        "        ################################\n",
        "        max_wav_value=32768.0,\n",
        "        sampling_rate=22050,\n",
        "        filter_length=1024,\n",
        "        hop_length=256,\n",
        "        win_length=1024,\n",
        "        n_mel_channels=80,\n",
        "        mel_fmin=0.0,\n",
        "        mel_fmax=8000.0,\n",
        "        f0_min=80,\n",
        "        f0_max=880,\n",
        "        harm_thresh=0.25,\n",
        "\n",
        "        ################################\n",
        "        # Model Parameters             #\n",
        "        ################################\n",
        "        n_symbols=len(symbols),\n",
        "        symbols_embedding_dim=512,\n",
        "\n",
        "        # Encoder parameters\n",
        "        encoder_kernel_size=5,\n",
        "        encoder_n_convolutions=3,\n",
        "        encoder_embedding_dim=512,\n",
        "\n",
        "        # Decoder parameters\n",
        "        n_frames_per_step=1,  # currently only 1 is supported\n",
        "        decoder_rnn_dim=1024,\n",
        "        prenet_dim=256,\n",
        "        prenet_f0_n_layers=1,\n",
        "        prenet_f0_dim=1,\n",
        "        prenet_f0_kernel_size=1,\n",
        "        prenet_rms_dim=0,\n",
        "        prenet_rms_kernel_size=1,\n",
        "        max_decoder_steps=1000,\n",
        "        gate_threshold=0.5,\n",
        "        p_attention_dropout=0.1,\n",
        "        p_decoder_dropout=0.1,\n",
        "        p_teacher_forcing=1.0,\n",
        "\n",
        "        # Attention parameters\n",
        "        attention_rnn_dim=1024,\n",
        "        attention_dim=128,\n",
        "\n",
        "        # Location Layer parameters\n",
        "        attention_location_n_filters=32,\n",
        "        attention_location_kernel_size=31,\n",
        "\n",
        "        # Mel-post processing network parameters\n",
        "        postnet_embedding_dim=512,\n",
        "        postnet_kernel_size=5,\n",
        "        postnet_n_convolutions=5,\n",
        "\n",
        "        # Speaker embedding\n",
        "        n_speakers=123,\n",
        "        speaker_embedding_dim=128,\n",
        "\n",
        "        # Reference encoder\n",
        "        with_gst=True,\n",
        "        ref_enc_filters=[32, 32, 64, 64, 128, 128],\n",
        "        ref_enc_size=[3, 3],\n",
        "        ref_enc_strides=[2, 2],\n",
        "        ref_enc_pad=[1, 1],\n",
        "        ref_enc_gru_size=128,\n",
        "\n",
        "        # Style Token Layer\n",
        "        token_embedding_size=256,\n",
        "        token_num=10,\n",
        "        num_heads=8,\n",
        "\n",
        "        ################################\n",
        "        # Optimization Hyperparameters #\n",
        "        ################################\n",
        "        use_saved_learning_rate=False,\n",
        "        learning_rate=1e-3,\n",
        "        learning_rate_min=1e-5,\n",
        "        learning_rate_anneal=50000,\n",
        "        weight_decay=1e-6,\n",
        "        grad_clip_thresh=1.0,\n",
        "        batch_size=32,\n",
        "        mask_padding=True,  # set model's padded outputs to padded values\n",
        "\n",
        "    )\n",
        "\n",
        "    if hparams_string:\n",
        "        tf.compat.v1.logging.info('Parsing command line hparams: %s', hparams_string)\n",
        "        hparams.parse(hparams_string)\n",
        "\n",
        "    if verbose:\n",
        "        tf.compat.v1.logging.info('Final parsed hparams: %s', hparams.values())\n",
        "\n",
        "    return hparams\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "GQbhux5TLLZj"
      },
      "source": [
        "#@title\n",
        "%%file train_utils.py\n",
        "import os\n",
        "import time\n",
        "import argparse\n",
        "import math\n",
        "from numpy import finfo\n",
        "\n",
        "import torch\n",
        "#from distributed import apply_gradient_allreduce\n",
        "import torch.distributed as dist\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from model import Tacotron2\n",
        "from data_utils import TextMelLoader, TextMelCollate\n",
        "from loss_function import Tacotron2Loss\n",
        "#from logger import Tacotron2Logger\n",
        "from hparams import create_hparams\n",
        "\n",
        "\n",
        "def reduce_tensor(tensor, n_gpus):\n",
        "    rt = tensor.clone()\n",
        "    dist.all_reduce(rt, op=dist.ReduceOp.SUM)\n",
        "    rt /= n_gpus\n",
        "    return rt\n",
        "\n",
        "\n",
        "def init_distributed(hparams, n_gpus, rank, group_name):\n",
        "    assert torch.cuda.is_available(), \"Distributed mode requires CUDA.\"\n",
        "    print(\"Initializing Distributed\")\n",
        "\n",
        "    # Set cuda device so everything is done on the right GPU.\n",
        "    torch.cuda.set_device(rank % torch.cuda.device_count())\n",
        "\n",
        "    # Initialize distributed communication\n",
        "    dist.init_process_group(\n",
        "        backend=hparams.dist_backend, init_method=hparams.dist_url,\n",
        "        world_size=n_gpus, rank=rank, group_name=group_name)\n",
        "\n",
        "    print(\"Done initializing distributed\")\n",
        "\n",
        "\n",
        "def prepare_dataloaders(hparams):\n",
        "    # Get data, data loaders and collate function ready\n",
        "    trainset = TextMelLoader(hparams.training_files, hparams)\n",
        "    valset = TextMelLoader(hparams.validation_files, hparams,\n",
        "                           speaker_ids=trainset.speaker_ids)\n",
        "    collate_fn = TextMelCollate(hparams.n_frames_per_step)\n",
        "\n",
        "    if hparams.distributed_run:\n",
        "        train_sampler = DistributedSampler(trainset)\n",
        "        shuffle = False\n",
        "    else:\n",
        "        train_sampler = None\n",
        "        shuffle = True\n",
        "\n",
        "    train_loader = DataLoader(trainset, num_workers=1, shuffle=shuffle,\n",
        "                              sampler=train_sampler,\n",
        "                              batch_size=hparams.batch_size, pin_memory=False,\n",
        "                              drop_last=True, collate_fn=collate_fn)\n",
        "    return train_loader, valset, collate_fn, train_sampler\n",
        "\n",
        "\n",
        "def prepare_directories_and_logger(output_directory, log_directory, rank):\n",
        "    if rank == 0:\n",
        "        if not os.path.isdir(output_directory):\n",
        "            os.makedirs(output_directory)\n",
        "            os.chmod(output_directory, 0o775)\n",
        "        logger = None    \n",
        "        #logger = Tacotron2Logger(os.path.join(output_directory, log_directory))\n",
        "    else:\n",
        "        logger = None\n",
        "    return logger\n",
        "\n",
        "\n",
        "def load_model(hparams):\n",
        "    model = Tacotron2(hparams).cuda()\n",
        "    if hparams.fp16_run:\n",
        "        model.decoder.attention_layer.score_mask_value = finfo('float16').min\n",
        "\n",
        "    if hparams.distributed_run:\n",
        "        model = apply_gradient_allreduce(model)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def warm_start_model(checkpoint_path, model, ignore_layers):\n",
        "    assert os.path.isfile(checkpoint_path)\n",
        "    print(\"Warm starting model from checkpoint '{}'\".format(checkpoint_path))\n",
        "    checkpoint_dict = torch.load(checkpoint_path, map_location='cpu')\n",
        "    model_dict = checkpoint_dict['state_dict']\n",
        "    if len(ignore_layers) > 0:\n",
        "        model_dict = {k: v for k, v in model_dict.items()\n",
        "                      if k not in ignore_layers}\n",
        "        dummy_dict = model.state_dict()\n",
        "        dummy_dict.update(model_dict)\n",
        "        model_dict = dummy_dict\n",
        "    model.load_state_dict(model_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "def load_checkpoint(checkpoint_path, model, optimizer):\n",
        "    assert os.path.isfile(checkpoint_path)\n",
        "    print(\"Loading checkpoint '{}'\".format(checkpoint_path))\n",
        "    checkpoint_dict = torch.load(checkpoint_path, map_location='cpu')\n",
        "    model.load_state_dict(checkpoint_dict['state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint_dict['optimizer'])\n",
        "    learning_rate = checkpoint_dict['learning_rate']\n",
        "    iteration = checkpoint_dict['iteration']\n",
        "    print(\"Loaded checkpoint '{}' from iteration {}\" .format(\n",
        "        checkpoint_path, iteration))\n",
        "    return model, optimizer, learning_rate, iteration\n",
        "\n",
        "\n",
        "def save_checkpoint(model, optimizer, learning_rate, iteration, filepath):\n",
        "    print(\"Saving model and optimizer state at iteration {} to {}\".format(\n",
        "        iteration, filepath))\n",
        "    torch.save({'iteration': iteration,\n",
        "                'state_dict': model.state_dict(),\n",
        "                'optimizer': optimizer.state_dict(),\n",
        "                'learning_rate': learning_rate}, filepath)\n",
        "\n",
        "\n",
        "def validate(model, criterion, valset, iteration, batch_size, n_gpus,\n",
        "             collate_fn, logger, distributed_run, rank):\n",
        "    \"\"\"Handles all the validation scoring and printing\"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_sampler = DistributedSampler(valset) if distributed_run else None\n",
        "        val_loader = DataLoader(valset, sampler=val_sampler, num_workers=1,\n",
        "                                shuffle=False, batch_size=batch_size,\n",
        "                                pin_memory=False, collate_fn=collate_fn)\n",
        "\n",
        "        val_loss = 0.0\n",
        "        for i, batch in enumerate(val_loader):\n",
        "            x, y = model.parse_batch(batch)\n",
        "            y_pred = model(x)\n",
        "            loss = criterion(y_pred, y)\n",
        "            if distributed_run:\n",
        "                reduced_val_loss = reduce_tensor(loss.data, n_gpus).item()\n",
        "            else:\n",
        "                reduced_val_loss = loss.item()\n",
        "            val_loss += reduced_val_loss\n",
        "        val_loss = val_loss / (i + 1)\n",
        "\n",
        "    model.train()\n",
        "    if rank == 0:\n",
        "        print(\"Validation loss {}: {:9f}  \".format(iteration, reduced_val_loss))\n",
        "        logger.log_validation(val_loss, model, y, y_pred, iteration)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "FTgQRjUqLNSr"
      },
      "source": [
        "#@title\n",
        "%%file /content/mellotron/mellotron_utils.py\n",
        "import re\n",
        "import numpy as np\n",
        "import music21 as m21\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from text import text_to_sequence, get_arpabet, cmudict\n",
        "\n",
        "\n",
        "CMUDICT_PATH = \"/content/mellotron/data/cmu_dictionary\"\n",
        "CMUDICT = cmudict.CMUDict(CMUDICT_PATH)\n",
        "PHONEME2GRAPHEME = {\n",
        "    'AA': ['a', 'o', 'ah'],\n",
        "    'AE': ['a', 'e'],\n",
        "    'AH': ['u', 'e', 'a', 'h', 'o'],\n",
        "    'AO': ['o', 'u', 'au'],\n",
        "    'AW': ['ou', 'ow'],\n",
        "    'AX': ['a'],\n",
        "    'AXR': ['er'],\n",
        "    'AY': ['i'],\n",
        "    'EH': ['e', 'ae'],\n",
        "    'EY': ['a', 'ai', 'ei', 'e', 'y'],\n",
        "    'IH': ['i', 'e'],\n",
        "    'IX': ['e', 'i'],\n",
        "    'IY': ['ea', 'ey', 'y', 'i'],\n",
        "    'OW': ['oa'],\n",
        "    'OY': ['oy'],\n",
        "    'UH': ['oo'],\n",
        "    'UW': ['oo', 'u', 'o'],\n",
        "    'UX': ['u'],\n",
        "    'B': ['b'],\n",
        "    'CH': ['ch', 'tch'],\n",
        "    'D': ['d', 'e', 'de'],\n",
        "    'DH': ['th'],\n",
        "    'DX': ['tt'],\n",
        "    'EL': ['le'],\n",
        "    'EM': ['m'],\n",
        "    'EN': ['on'],\n",
        "    'ER': ['i', 'er'],\n",
        "    'F': ['f'],\n",
        "    'G': ['g'],\n",
        "    'HH': ['h'],\n",
        "    'JH': ['j'],\n",
        "    'K': ['k', 'c', 'ch'],\n",
        "    'KS': ['x'],\n",
        "    'L': ['ll', 'l'],\n",
        "    'M': ['m'],\n",
        "    'N': ['n', 'gn'],\n",
        "    'NG': ['ng'],\n",
        "    'NX': ['nn'],\n",
        "    'P': ['p'],\n",
        "    'Q': ['-'],\n",
        "    'R': ['wr', 'r'],\n",
        "    'S': ['s', 'ce'],\n",
        "    'SH': ['sh'],\n",
        "    'T': ['t'],\n",
        "    'TH': ['th'],\n",
        "    'V': ['v', 'f', 'e'],\n",
        "    'W': ['w'],\n",
        "    'WH': ['wh'],\n",
        "    'Y': ['y', 'j'],\n",
        "    'Z': ['z', 's'],\n",
        "    'ZH': ['s']\n",
        "}\n",
        "\n",
        "########################\n",
        "#  CONSONANT DURATION  #\n",
        "########################\n",
        "PHONEMEDURATION = {\n",
        "    'B': 0.05,\n",
        "    'CH': 0.1,\n",
        "    'D': 0.075,\n",
        "    'DH': 0.05,\n",
        "    'DX': 0.05,\n",
        "    'EL': 0.05,\n",
        "    'EM': 0.05,\n",
        "    'EN': 0.05,\n",
        "    'F': 0.1,\n",
        "    'G': 0.05,\n",
        "    'HH': 0.05,\n",
        "    'JH': 0.05,\n",
        "    'K': 0.05,\n",
        "    'L': 0.05,\n",
        "    'M': 0.15,\n",
        "    'N': 0.15,\n",
        "    'NG': 0.15,\n",
        "    'NX': 0.05,\n",
        "    'P': 0.05,\n",
        "    'Q': 0.075,\n",
        "    'R': 0.05,\n",
        "    'S': 0.1,\n",
        "    'SH': 0.05,\n",
        "    'T': 0.075,\n",
        "    'TH': 0.1,\n",
        "    'V': 0.05,\n",
        "    'Y': 0.05,\n",
        "    'W': 0.05,\n",
        "    'WH': 0.05,\n",
        "    'Z': 0.05,\n",
        "    'ZH': 0.05\n",
        "}\n",
        "\n",
        "\n",
        "def add_space_between_events(events, connect=False):\n",
        "    new_events = []\n",
        "    for i in range(1, len(events)):\n",
        "        token_a, freq_a, start_time_a, end_time_a = events[i-1][-1]\n",
        "        token_b, freq_b, start_time_b, end_time_b = events[i][0]\n",
        "\n",
        "        if token_a in (' ', '') and len(events[i-1]) == 1:\n",
        "            new_events.append(events[i-1])\n",
        "        elif token_a not in (' ', '') and token_b not in (' ', ''):\n",
        "            new_events.append(events[i-1])\n",
        "            if connect:\n",
        "                new_events.append([[' ', 0, end_time_a, start_time_b]])\n",
        "            else:\n",
        "                new_events.append([[' ', 0, end_time_a, end_time_a]])\n",
        "        else:\n",
        "            new_events.append(events[i-1])\n",
        "\n",
        "    if new_events[-1][0][0] != ' ':\n",
        "        new_events.append([[' ', 0, end_time_a, end_time_a]])\n",
        "    new_events.append(events[-1])\n",
        "\n",
        "    return new_events\n",
        "\n",
        "\n",
        "def adjust_words(events):\n",
        "    new_events = []\n",
        "    for event in events:\n",
        "        if len(event) == 1 and event[0][0] == ' ':\n",
        "            new_events.append(event)\n",
        "        else:\n",
        "            for e in event:\n",
        "                if e[0][0].isupper():\n",
        "                    new_events.append([e])\n",
        "                else:\n",
        "                    new_events[-1].extend([e])\n",
        "    return new_events\n",
        "\n",
        "\n",
        "def adjust_extensions(events, phoneme_durations):\n",
        "    if len(events) == 1:\n",
        "        return events\n",
        "\n",
        "    idx_last_vowel = None\n",
        "    n_consonants_after_last_vowel = 0\n",
        "    target_ids = np.arange(len(events))\n",
        "    for i in range(len(events)):\n",
        "        token = re.sub('[0-9{}]', '', events[i][0])\n",
        "        if idx_last_vowel is None and token not in phoneme_durations:\n",
        "            idx_last_vowel = i\n",
        "            n_consonants_after_last_vowel = 0\n",
        "        else:\n",
        "            if token == '_' and not n_consonants_after_last_vowel:\n",
        "                events[i][0] = events[idx_last_vowel][0]\n",
        "            elif token == '_' and n_consonants_after_last_vowel:\n",
        "                events[i][0] = events[idx_last_vowel][0]\n",
        "                start = idx_last_vowel + 1\n",
        "                target_ids[start:start+n_consonants_after_last_vowel] += 1\n",
        "                target_ids[i] -= n_consonants_after_last_vowel\n",
        "            elif token in phoneme_durations:\n",
        "                n_consonants_after_last_vowel += 1\n",
        "            else:\n",
        "                n_consonants_after_last_vowel = 0\n",
        "                idx_last_vowel = i\n",
        "\n",
        "    new_events = [0] * len(events)\n",
        "    for i in range(len(events)):\n",
        "        new_events[target_ids[i]] = events[i]\n",
        "\n",
        "    # adjust time of consonants that were repositioned\n",
        "    for i in range(1, len(new_events)):\n",
        "        if new_events[i][2] < new_events[i-1][2]:\n",
        "            new_events[i][2] = new_events[i-1][2]\n",
        "            new_events[i][3] = new_events[i-1][3]\n",
        "\n",
        "    return new_events\n",
        "\n",
        "\n",
        "def adjust_consonant_lengths(events, phoneme_durations):\n",
        "    t_init = events[0][2]\n",
        "\n",
        "    idx_last_vowel = None\n",
        "    for i in range(len(events)):\n",
        "        task = re.sub('[0-9{}]', '', events[i][0])\n",
        "        if task in phoneme_durations:\n",
        "            duration = phoneme_durations[task]\n",
        "            if idx_last_vowel is None:  # consonant comes before any vowel\n",
        "                events[i][2] = t_init\n",
        "                events[i][3] = t_init + duration\n",
        "            else:  # consonant comes after a vowel, must offset\n",
        "                events[idx_last_vowel][3] -= duration\n",
        "                for k in range(idx_last_vowel+1, i):\n",
        "                    events[k][2] -= duration\n",
        "                    events[k][3] -= duration\n",
        "                events[i][2] = events[i-1][3]\n",
        "                events[i][3] = events[i-1][3] + duration\n",
        "        else:\n",
        "            events[i][2] = t_init\n",
        "            events[i][3] = events[i][3]\n",
        "            t_init = events[i][3]\n",
        "            idx_last_vowel = i\n",
        "        t_init = events[i][3]\n",
        "\n",
        "    return events\n",
        "\n",
        "\n",
        "def adjust_consonants(events, phoneme_durations):\n",
        "    if len(events) == 1:\n",
        "        return events\n",
        "\n",
        "    start = 0\n",
        "    split_ids = []\n",
        "    t_init = events[0][2]\n",
        "\n",
        "    # get each substring group\n",
        "    for i in range(1, len(events)):\n",
        "        if events[i][2] != t_init:\n",
        "            split_ids.append((start, i))\n",
        "            start = i\n",
        "            t_init = events[i][2]\n",
        "    split_ids.append((start, len(events)))\n",
        "\n",
        "    for (start, end) in split_ids:\n",
        "        events[start:end] = adjust_consonant_lengths(\n",
        "            events[start:end], phoneme_durations)\n",
        "\n",
        "    return events\n",
        "\n",
        "\n",
        "def adjust_event(event, hop_length=256, sampling_rate=22050):\n",
        "    tokens, freq, start_time, end_time = event\n",
        "\n",
        "    if tokens == ' ':\n",
        "        return [event] if freq == 0 else [['_', freq, start_time, end_time]]\n",
        "\n",
        "    return [[token, freq, start_time, end_time] for token in tokens]\n",
        "\n",
        "\n",
        "def musicxml2score(filepath, bpm=60):\n",
        "    track = {}\n",
        "    beat_length_seconds = 60/bpm\n",
        "    data = m21.converter.parse(filepath)\n",
        "    for i in range(len(data.parts)):\n",
        "        part = data.parts[i].flat\n",
        "        events = []\n",
        "        for k in range(len(part.notesAndRests)):\n",
        "            event = part.notesAndRests[k]\n",
        "            if isinstance(event, m21.note.Note):\n",
        "                freq = event.pitch.frequency\n",
        "                token = event.lyrics[0].text if len(event.lyrics) > 0 else ' '\n",
        "                start_time = event.offset * beat_length_seconds\n",
        "                end_time = start_time + event.duration.quarterLength * beat_length_seconds\n",
        "                event = [token, freq, start_time, end_time]\n",
        "            elif isinstance(event, m21.note.Rest):\n",
        "                freq = 0\n",
        "                token = ' '\n",
        "                start_time = event.offset * beat_length_seconds\n",
        "                end_time = start_time + event.duration.quarterLength * beat_length_seconds\n",
        "                event = [token, freq, start_time, end_time]\n",
        "\n",
        "            if token == '_':\n",
        "                raise Exception(\"Unexpected token {}\".format(token))\n",
        "\n",
        "            if len(events) == 0:\n",
        "                events.append(event)\n",
        "            else:\n",
        "                if token == ' ':\n",
        "                    if freq == 0:\n",
        "                        if events[-1][1] == 0:\n",
        "                            events[-1][3] = end_time\n",
        "                        else:\n",
        "                            events.append(event)\n",
        "                    elif freq == events[-1][1]:  # is event duration extension ?\n",
        "                        events[-1][-1] = end_time\n",
        "                    else:  # must be different note on same syllable\n",
        "                        events.append(event)\n",
        "                else:\n",
        "                    events.append(event)\n",
        "        track[part.partName] = events\n",
        "    return track\n",
        "\n",
        "\n",
        "def track2events(track):\n",
        "    events = []\n",
        "    for e in track:\n",
        "        events.extend(adjust_event(e))\n",
        "    group_ids = [i for i in range(len(events))\n",
        "                 if events[i][0] in [' '] or events[i][0].isupper()]\n",
        "\n",
        "    events_grouped = []\n",
        "    for i in range(1, len(group_ids)):\n",
        "        start, end = group_ids[i-1], group_ids[i]\n",
        "        events_grouped.append(events[start:end])\n",
        "\n",
        "    if events[-1][0] != ' ':\n",
        "        events_grouped.append(events[group_ids[-1]:])\n",
        "\n",
        "    return events_grouped\n",
        "\n",
        "\n",
        "def events2eventsarpabet(event):\n",
        "    if event[0][0] == ' ':\n",
        "        return event\n",
        "\n",
        "    # get word and word arpabet\n",
        "    word = ''.join([e[0] for e in event if e[0] not in('_', ' ')])\n",
        "    word_arpabet = get_arpabet(word, CMUDICT)\n",
        "    if word_arpabet[0] != '{':\n",
        "        return event\n",
        "\n",
        "    word_arpabet = word_arpabet.split()\n",
        "\n",
        "    # align tokens to arpabet\n",
        "    i, k = 0, 0\n",
        "    new_events = []\n",
        "    while i < len(event) and k < len(word_arpabet):\n",
        "        # single token\n",
        "        token_a, freq_a, start_time_a, end_time_a = event[i]\n",
        "\n",
        "        if token_a == ' ':\n",
        "            new_events.append([token_a, freq_a, start_time_a, end_time_a])\n",
        "            i += 1\n",
        "            continue\n",
        "\n",
        "        if token_a == '_':\n",
        "            new_events.append([token_a, freq_a, start_time_a, end_time_a])\n",
        "            i += 1\n",
        "            continue\n",
        "\n",
        "        # two tokens\n",
        "        if i < len(event) - 1:\n",
        "            j = i + 1\n",
        "            token_b, freq_b, start_time_b, end_time_b = event[j]\n",
        "            between_events = []\n",
        "            while j < len(event) and event[j][0] == '_':\n",
        "                between_events.append([token_b, freq_b, start_time_b, end_time_b])\n",
        "                j += 1\n",
        "                if j < len(event):\n",
        "                    token_b, freq_b, start_time_b, end_time_b = event[j]\n",
        "\n",
        "            token_compound_2 = (token_a + token_b).lower()\n",
        "\n",
        "        # single arpabet\n",
        "        arpabet = re.sub('[0-9{}]', '', word_arpabet[k])\n",
        "\n",
        "        if k < len(word_arpabet) - 1:\n",
        "            arpabet_compound_2 = ''.join(word_arpabet[k:k+2])\n",
        "            arpabet_compound_2 = re.sub('[0-9{}]', '', arpabet_compound_2)\n",
        "\n",
        "        if i < len(event) - 1 and token_compound_2 in PHONEME2GRAPHEME[arpabet]:\n",
        "            new_events.append([word_arpabet[k], freq_a, start_time_a, end_time_a])\n",
        "            if len(between_events):\n",
        "                new_events.extend(between_events)\n",
        "            if start_time_a != start_time_b:\n",
        "                new_events.append([word_arpabet[k], freq_b, start_time_b, end_time_b])\n",
        "            i += 2 + len(between_events)\n",
        "            k += 1\n",
        "        elif token_a.lower() in PHONEME2GRAPHEME[arpabet]:\n",
        "            new_events.append([word_arpabet[k], freq_a, start_time_a, end_time_a])\n",
        "            i += 1\n",
        "            k += 1\n",
        "        elif arpabet_compound_2 in PHONEME2GRAPHEME and token_a.lower() in PHONEME2GRAPHEME[arpabet_compound_2]:\n",
        "            new_events.append([word_arpabet[k], freq_a, start_time_a, end_time_a])\n",
        "            new_events.append([word_arpabet[k+1], freq_a, start_time_a, end_time_a])\n",
        "            i += 1\n",
        "            k += 2\n",
        "        else:\n",
        "            k += 1\n",
        "\n",
        "    if len(event) == 1:\n",
        "        if len(word_arpabet) > 1:\n",
        "            raise Exception(\"More characters in {} than in {}\".format(\n",
        "                word, word_arpabet))\n",
        "        token_a, freq_a, start_time_a, end_time_a = event[i]\n",
        "        new_events.append([word_arpabet[0], freq_a, start_time_a, end_time_a])\n",
        "\n",
        "    # add extensions and pauses at end of words\n",
        "    while i < len(event):\n",
        "        token_a, freq_a, start_time_a, end_time_a = event[i]\n",
        "\n",
        "        if token_a in (' ', '_'):\n",
        "            new_events.append([token_a, freq_a, start_time_a, end_time_a])\n",
        "        i += 1\n",
        "\n",
        "    return new_events\n",
        "\n",
        "\n",
        "def event2alignment(events, hop_length=256, sampling_rate=22050):\n",
        "    frame_length = float(hop_length) / float(sampling_rate)\n",
        "\n",
        "    n_frames = int(events[-1][-1][-1] / frame_length)\n",
        "    n_tokens = np.sum([len(e) for e in events])\n",
        "    alignment = np.zeros((n_tokens, n_frames))\n",
        "\n",
        "    cur_event = -1\n",
        "    for event in events:\n",
        "        for i in range(len(event)):\n",
        "            if len(event) == 1 or cur_event == -1 or event[i][0] != event[i-1][0]:\n",
        "                cur_event += 1\n",
        "            token, freq, start_time, end_time = event[i]\n",
        "            alignment[cur_event, int(start_time/frame_length):int(end_time/frame_length)] = 1\n",
        "\n",
        "    return alignment[:cur_event+1]\n",
        "\n",
        "\n",
        "def event2f0(events, hop_length=256, sampling_rate=22050):\n",
        "    frame_length = float(hop_length) / float(sampling_rate)\n",
        "    n_frames = int(events[-1][-1][-1] / frame_length)\n",
        "    f0s = np.zeros((1, n_frames))\n",
        "\n",
        "    for event in events:\n",
        "        for i in range(len(event)):\n",
        "            token, freq, start_time, end_time = event[i]\n",
        "            f0s[0, int(start_time/frame_length):int(end_time/frame_length)] = freq\n",
        "\n",
        "    return f0s\n",
        "\n",
        "\n",
        "def event2text(events, convert_stress, cmudict=None):\n",
        "    text_clean = ''\n",
        "    for event in events:\n",
        "        for i in range(len(event)):\n",
        "            if i > 0 and event[i][0] == event[i-1][0]:\n",
        "                continue\n",
        "            if event[i][0] == ' ' and len(event) > 1:\n",
        "                if text_clean[-1] != \"}\":\n",
        "                    text_clean = text_clean[:-1] + '} {'\n",
        "                else:\n",
        "                    text_clean += ' {'\n",
        "            else:\n",
        "                if event[i][0][-1] in ('}', ' '):\n",
        "                    text_clean += event[i][0]\n",
        "                else:\n",
        "                    text_clean += event[i][0] + ' '\n",
        "\n",
        "    if convert_stress:\n",
        "        text_clean = re.sub('[0-9]', '1', text_clean)\n",
        "\n",
        "    text_encoded = text_to_sequence(text_clean, [], cmudict)\n",
        "    return text_encoded, text_clean\n",
        "\n",
        "\n",
        "def remove_excess_frames(alignment, f0s):\n",
        "    excess_frames = np.sum(alignment.sum(0) == 0)\n",
        "    alignment = alignment[:, :-excess_frames] if excess_frames > 0 else alignment\n",
        "    f0s = f0s[:, :-excess_frames] if excess_frames > 0 else f0s\n",
        "    return alignment, f0s\n",
        "\n",
        "\n",
        "def get_data_from_musicxml(filepath, bpm, phoneme_durations=None,\n",
        "                           convert_stress=False):\n",
        "    if phoneme_durations is None:\n",
        "        phoneme_durations = PHONEMEDURATION\n",
        "    score = musicxml2score(filepath, bpm)\n",
        "    data = {}\n",
        "    for k, v in score.items():\n",
        "        # ignore empty tracks\n",
        "        if len(v) == 1 and v[0][0] == ' ':\n",
        "            continue\n",
        "\n",
        "        events = track2events(v)\n",
        "        events = adjust_words(events)\n",
        "        events_arpabet = [events2eventsarpabet(e) for e in events]\n",
        "\n",
        "        # make adjustments\n",
        "        events_arpabet = [adjust_extensions(e, phoneme_durations)\n",
        "                          for e in events_arpabet]\n",
        "        events_arpabet = [adjust_consonants(e, phoneme_durations)\n",
        "                          for e in events_arpabet]\n",
        "        events_arpabet = add_space_between_events(events_arpabet)\n",
        "\n",
        "        # convert data to alignment, f0 and text encoded\n",
        "        alignment = event2alignment(events_arpabet)\n",
        "        f0s = event2f0(events_arpabet)\n",
        "        alignment, f0s = remove_excess_frames(alignment, f0s)\n",
        "        text_encoded, text_clean = event2text(events_arpabet, convert_stress)\n",
        "\n",
        "        # convert data to torch\n",
        "        alignment = torch.from_numpy(alignment).permute(1, 0)[:, None].float()\n",
        "        f0s = torch.from_numpy(f0s)[None].float()\n",
        "        text_encoded = torch.LongTensor(text_encoded)[None]\n",
        "        data[k] = {'rhythm': alignment,\n",
        "                   'pitch_contour': f0s,\n",
        "                   'text_encoded': text_encoded}\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import argparse\n",
        "    # Get defaults so it can work with no Sacred\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('-f', \"--filepath\", required=True)\n",
        "    args = parser.parse_args()\n",
        "    get_data_from_musicxml(args.filepath, 60)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuiBQph4LPTL"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import IPython.display as ipd\n",
        "\n",
        "import sys\n",
        "################################################################################################\n",
        "sys.path.append('/content/mellotron')             #####  modified for  colab ######\n",
        "sys.path.append('/content/mellotron/waveglow/')   #####  modified for  colab ######\n",
        "################################################################################################\n",
        "\n",
        "from itertools import cycle\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "from scipy.io.wavfile import write\n",
        "import pandas as pd\n",
        "import librosa\n",
        "import torch\n",
        "\n",
        "from hparams import create_hparams\n",
        "from model import Tacotron2\n",
        "from waveglow.denoiser import Denoiser\n",
        "from layers import TacotronSTFT\n",
        "################################################################################################\n",
        "from train_utils import load_model #####  modified for inference  on colab #####################\n",
        "################################################################################################\n",
        "from data_utils import TextMelLoader, TextMelCollate\n",
        "from text import cmudict, text_to_sequence\n",
        "from mellotron_utils import get_data_from_musicxml"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3kHSCdmLRGJ"
      },
      "source": [
        "def panner(signal, angle):\n",
        "    angle = np.radians(angle)\n",
        "    left = np.sqrt(2)/2.0 * (np.cos(angle) - np.sin(angle)) * signal\n",
        "    right = np.sqrt(2)/2.0 * (np.cos(angle) + np.sin(angle)) * signal\n",
        "    return np.dstack((left, right))[0]\n",
        "\n",
        "def plot_mel_f0_alignment(mel_source, mel_outputs_postnet, f0s, alignments, figsize=(16, 16)):\n",
        "    fig, axes = plt.subplots(4, 1, figsize=figsize)\n",
        "    axes = axes.flatten()\n",
        "    axes[0].imshow(mel_source, aspect='auto', origin='bottom', interpolation='none')\n",
        "    axes[1].imshow(mel_outputs_postnet, aspect='auto', origin='bottom', interpolation='none')\n",
        "    axes[2].scatter(range(len(f0s)), f0s, alpha=0.5, color='red', marker='.', s=1)\n",
        "    axes[2].set_xlim(0, len(f0s))\n",
        "    axes[3].imshow(alignments, aspect='auto', origin='bottom', interpolation='none')\n",
        "    axes[0].set_title(\"Source Mel\")\n",
        "    axes[1].set_title(\"Predicted Mel\")\n",
        "    axes[2].set_title(\"Source pitch contour\")\n",
        "    axes[3].set_title(\"Source rhythm\")\n",
        "    plt.tight_layout()\n",
        "\n",
        "def load_mel(path):\n",
        "    audio, sampling_rate = librosa.core.load(path, sr=hparams.sampling_rate)\n",
        "    audio = torch.from_numpy(audio)\n",
        "    if sampling_rate != hparams.sampling_rate:\n",
        "        raise ValueError(\"{} SR doesn't match target {} SR\".format(\n",
        "            sampling_rate, stft.sampling_rate))\n",
        "    audio_norm = audio / hparams.max_wav_value\n",
        "    audio_norm = audio_norm.unsqueeze(0)\n",
        "    audio_norm = torch.autograd.Variable(audio_norm, requires_grad=False)\n",
        "    melspec = stft.mel_spectrogram(audio_norm)\n",
        "    melspec = melspec.cuda()\n",
        "    return melspec\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUJXSRWTLSok"
      },
      "source": [
        "hparams = create_hparams()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vrso6Y_ALTVx"
      },
      "source": [
        "stft = TacotronSTFT(hparams.filter_length, hparams.hop_length, hparams.win_length,\n",
        "                    hparams.n_mel_channels, hparams.sampling_rate, hparams.mel_fmin,\n",
        "                    hparams.mel_fmax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mpf4PnW_LVCY"
      },
      "source": [
        "**Step 3.** Load the models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFTOrikBLXX-"
      },
      "source": [
        "checkpoint_path = \"/content/mellotron_libritts.pt\"\n",
        "tacotron = load_model(hparams).cuda().eval()\n",
        "tacotron.load_state_dict(torch.load(checkpoint_path)['state_dict'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bscqTgsQLYou"
      },
      "source": [
        "waveglow_path = '/content/waveglow_256channels_universal_v5.pt' \n",
        "waveglow = torch.load(waveglow_path)['model'].cuda().eval()\n",
        "denoiser = Denoiser(waveglow).cuda().eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqXqQF6gLamt"
      },
      "source": [
        "**Step 4a.** Provide target audio files for pitch and rhythm! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Thd_h-olV9-R"
      },
      "source": [
        "%cd mellotron/data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHHsgCYxSFsH"
      },
      "source": [
        "import requests\n",
        "#download target audio file\n",
        "url = 'https://github.com/mitmedialab/MAS.S60.Fall2020/blob/master/homework/hw3-files/thenthenorthwindowblewashardashecould.wav?raw=true'\n",
        "r = requests.get(url, allow_redirects=True)\n",
        "open('thenorthwindblewashardashecould.wav', 'wb').write(r.content)\n",
        "\n",
        "#download target audio file\n",
        "url = 'https://github.com/mitmedialab/MAS.S60.Fall2020/blob/master/homework/hw3-files/remembertoletherintoyourheart.wav?raw=true'\n",
        "r = requests.get(url, allow_redirects=True)\n",
        "open('remembertoletherintoyourheart.wav', 'wb').write(r.content)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4v7vA4awM4Is"
      },
      "source": [
        "%ls\n",
        "%cd ../../"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WK3F_m1uLgkv"
      },
      "source": [
        "%%file /content/mellotron/data/examples_filelist.txt\n",
        "/content/mellotron/data/example1.wav|exploring the expanses of space to keep our planet safe|1\n",
        "/content/mellotron/data/example2.wav|and all the species that call it home|1\n",
        "/content/mellotron/data/thenorthwindblewashardashecould.wav|then the north wind blew as hard as he could|1\n",
        "/content/mellotron/data/remembertoletherintoyourheart.wav|remember to let her into your heart|1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShrjzmkuLi7r"
      },
      "source": [
        "arpabet_dict = cmudict.CMUDict('/content/mellotron/data/cmu_dictionary')\n",
        "audio_paths = '/content/mellotron/data/examples_filelist.txt'\n",
        "dataloader = TextMelLoader(audio_paths, hparams)\n",
        "datacollate = TextMelCollate(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYO4_U3qWtNR"
      },
      "source": [
        "%ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtjABbuULj13"
      },
      "source": [
        "**Step 5.** Load the target file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AnTaVZ0e8nj"
      },
      "source": [
        "for idx in range(len(dataloader)): \n",
        "  audio_path, text, sid = dataloader.audiopaths_and_text[idx]\n",
        "  print(\"file_idx =\" + str(idx) + \":\" + audio_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2walt8LEhWjI"
      },
      "source": [
        "Select the file you want to use as the target. Note the index of the file above. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ID6SsRMgLmrI"
      },
      "source": [
        "#choose the audio file\n",
        "file_idx = 0\n",
        "\n",
        "audio_path, text, sid = dataloader.audiopaths_and_text[file_idx]\n",
        "print(audio_path)\n",
        "\n",
        "print(len(dataloader.audiopaths_and_text))\n",
        "\n",
        "# get audio path, encoded text, pitch contour and mel for gst\n",
        "text_encoded = torch.LongTensor(text_to_sequence(text, hparams.text_cleaners, arpabet_dict))[None, :].cuda()    \n",
        "pitch_contour = dataloader[file_idx][3][None].cuda()\n",
        "mel = load_mel(audio_path)\n",
        "print(audio_path, text)\n",
        "\n",
        "# load source data to obtain rhythm using tacotron 2 as a forced aligner\n",
        "x, y = tacotron.parse_batch(datacollate([dataloader[file_idx]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iu8URPOjLpPt"
      },
      "source": [
        "ipd.Audio(audio_path, rate=hparams.sampling_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rx8rOu9SLqin"
      },
      "source": [
        "**Step 6.** Define the speakers set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1Uj50UMLtIv"
      },
      "source": [
        "speaker_ids = TextMelLoader(\"/content/mellotron/filelists/libritts_train_clean_100_audiopath_text_sid_atleast5min_val_filelist.txt\", hparams).speaker_ids\n",
        "speakers = pd.read_csv('/content/mellotron/filelists/libritts_speakerinfo.txt', engine='python',header=None, comment=';', sep=' *\\| *', \n",
        "                       names=['ID', 'SEX', 'SUBSET', 'MINUTES', 'NAME'])\n",
        "speakers['MELLOTRON_ID'] = speakers['ID'].apply(lambda x: speaker_ids[x] if x in speaker_ids else -1)\n",
        "female_speakers = cycle(\n",
        "    speakers.query(\"SEX == 'F' and MINUTES > 20 and MELLOTRON_ID >= 0\")['MELLOTRON_ID'].sample(frac=1).tolist())\n",
        "male_speakers = cycle(\n",
        "    speakers.query(\"SEX == 'M' and MINUTES > 20 and MELLOTRON_ID >= 0\")['MELLOTRON_ID'].sample(frac=1).tolist())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQvoZEinLumC"
      },
      "source": [
        "**Step 7.** Style Transfer (Rhythm and Pitch Contour). The speaker is chosen by random by default. The generated voice will speak at the same rythym and pitch as you. Note that if your pitch is out of rane for the speaker, it will be clipped to speak at it's maximum or minimum pitch. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HH2xDCfyLxgb"
      },
      "source": [
        "with torch.no_grad():\n",
        "    # get rhythm (alignment map) using tacotron 2\n",
        "    mel_outputs, mel_outputs_postnet, gate_outputs, rhythm = tacotron.forward(x)\n",
        "    rhythm = rhythm.permute(1, 0, 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZyQ_kPEL49I"
      },
      "source": [
        "# Note - Now a speaker is chosen \n",
        "#This is where a speaker is chosen from a library. They are made to say what you said in your rhythm and pitch! We are choosing them randomly.\n",
        "\n",
        "speaker_id = next(female_speakers) if np.random.randint(2) else next(male_speakers)\n",
        "speaker_id = torch.LongTensor([speaker_id]).cuda()\n",
        "\n",
        "with torch.no_grad():\n",
        "    mel_outputs, mel_outputs_postnet, gate_outputs, _ = tacotron.inference_noattention(\n",
        "        (text_encoded, mel, speaker_id, pitch_contour, rhythm))\n",
        "\n",
        "plot_mel_f0_alignment(x[2].data.cpu().numpy()[0],\n",
        "                      mel_outputs_postnet.data.cpu().numpy()[0],\n",
        "                      pitch_contour.data.cpu().numpy()[0, 0],\n",
        "                      rhythm.data.cpu().numpy()[:, 0].T)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UJ_Mzo8L66T"
      },
      "source": [
        "with torch.no_grad():\n",
        "    audio = denoiser(waveglow.infer(mel_outputs_postnet, sigma=0.8), 0.01)[:, 0]\n",
        "ipd.Audio(audio[0].data.cpu().numpy(), rate=hparams.sampling_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HEmYsmmL7x3"
      },
      "source": [
        "**Try another random voice.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXgBRsENL-dk"
      },
      "source": [
        "speaker_id = next(female_speakers) if np.random.randint(2) else next(male_speakers)\n",
        "speaker_id = torch.LongTensor([speaker_id]).cuda()\n",
        "\n",
        "with torch.no_grad():\n",
        "    mel_outputs, mel_outputs_postnet, gate_outputs, _ = tacotron.inference_noattention(\n",
        "        (text_encoded, mel, speaker_id, pitch_contour, rhythm))\n",
        "\n",
        "plot_mel_f0_alignment(x[2].data.cpu().numpy()[0],\n",
        "                      mel_outputs_postnet.data.cpu().numpy()[0],\n",
        "                      pitch_contour.data.cpu().numpy()[0, 0],\n",
        "                      rhythm.data.cpu().numpy()[:, 0].T)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StmD3JKXMAV_"
      },
      "source": [
        "with torch.no_grad():\n",
        "    audio = denoiser(waveglow.infer(mel_outputs_postnet, sigma=0.9), 0.01)[:, 0]\n",
        "ipd.Audio(audio[0].data.cpu().numpy(), rate=hparams.sampling_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0jo50PKR3yb"
      },
      "source": [
        "##TODO: Try with your own audio file. \n",
        "\n",
        "**Upload an audio file.** This is where you can include the source for the rhythm and pitch.\n",
        "\n",
        "*   Add your wav file (22050 sampling rate) to the folder `/content/mellotron/data/your_audio_file.wav.` You can use the uploader below. \n",
        "*   Write a transcription for your uploaded file. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRicZiLTPiSh"
      },
      "source": [
        "#Upload a wav file to /content/mellotron/data/\n",
        "\n",
        "%cd mellotron/data/\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "uploaded_file = list(uploaded.keys())[0] \n",
        "\n",
        "%cd ../../"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMEih3NsjJBT"
      },
      "source": [
        "**Write a transcription for what you say.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jTNb4e0QgwG"
      },
      "source": [
        "transcription_text = 'and another one bites the dust' #@param {type:\"string\"}\n",
        "\n",
        "newline = \"/content/mellotron/data/\"+uploaded_file+\"|\"+transcription_text+\"|1\"\n",
        "\n",
        "with open('/content/mellotron/data/examples_filelist.txt', 'a') as writefile:\n",
        "    writefile.write(\"\\n\")\n",
        "    writefile.write(newline)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZxr11OQkQDv"
      },
      "source": [
        "arpabet_dict = cmudict.CMUDict('/content/mellotron/data/cmu_dictionary')\n",
        "audio_paths = '/content/mellotron/data/examples_filelist.txt'\n",
        "dataloader = TextMelLoader(audio_paths, hparams)\n",
        "datacollate = TextMelCollate(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nKRq9TTkBBM"
      },
      "source": [
        "for idx in range(len(dataloader)): \n",
        "  audio_path, text, sid = dataloader.audiopaths_and_text[idx]\n",
        "  print(\"file_idx =\" + str(idx) + \":\" + audio_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3bET8nZj_2_"
      },
      "source": [
        "#choose the audio file\n",
        "file_idx = 2\n",
        "\n",
        "audio_path, text, sid = dataloader.audiopaths_and_text[file_idx]\n",
        "print(audio_path)\n",
        "\n",
        "print(len(dataloader.audiopaths_and_text))\n",
        "\n",
        "# get audio path, encoded text, pitch contour and mel for gst\n",
        "text_encoded = torch.LongTensor(text_to_sequence(text, hparams.text_cleaners, arpabet_dict))[None, :].cuda()    \n",
        "pitch_contour = dataloader[file_idx][3][None].cuda()\n",
        "mel = load_mel(audio_path)\n",
        "print(audio_path, text)\n",
        "\n",
        "# load source data to obtain rhythm using tacotron 2 as a forced aligner\n",
        "x, y = tacotron.parse_batch(datacollate([dataloader[file_idx]]))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yE5EVA_-kD0A"
      },
      "source": [
        "ipd.Audio(audio_path, rate=hparams.sampling_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBC36HmAkyXG"
      },
      "source": [
        "with torch.no_grad():\n",
        "    # get rhythm (alignment map) using tacotron 2\n",
        "    mel_outputs, mel_outputs_postnet, gate_outputs, rhythm = tacotron.forward(x)\n",
        "    rhythm = rhythm.permute(1, 0, 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVs-s0mwijgm"
      },
      "source": [
        "speaker_id = next(female_speakers) if np.random.randint(2) else next(male_speakers)\n",
        "speaker_id = torch.LongTensor([speaker_id]).cuda()\n",
        "\n",
        "with torch.no_grad():\n",
        "    mel_outputs, mel_outputs_postnet, gate_outputs, _ = tacotron.inference_noattention(\n",
        "        (text_encoded, mel, speaker_id, pitch_contour, rhythm))\n",
        "\n",
        "plot_mel_f0_alignment(x[2].data.cpu().numpy()[0],\n",
        "                      mel_outputs_postnet.data.cpu().numpy()[0],\n",
        "                      pitch_contour.data.cpu().numpy()[0, 0],\n",
        "                      rhythm.data.cpu().numpy()[:, 0].T)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjGG7_XaikGq"
      },
      "source": [
        "with torch.no_grad():\n",
        "    audio = denoiser(waveglow.infer(mel_outputs_postnet, sigma=0.9), 0.01)[:, 0]\n",
        "ipd.Audio(audio[0].data.cpu().numpy(), rate=hparams.sampling_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xpv3iov8sN2d"
      },
      "source": [
        "# Other Interesting Reads: \n",
        "https://towardsdatascience.com/how-i-made-one-of-the-worlds-first-100-ai-songs-45da7297075c\n"
      ]
    }
  ]
}