{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Experiments in Synthetic Media_Experiment 1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "-CvK7Y9N3jye"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TS6c9_lDGyY"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\" style=\"text-align: center;\">\n",
        "<a href=\"https://colab.research.google.com/drive/12_Z4zQy-23HEyL4a373mx0p3tpX7eUi-#scrollTo=0vP9EENLd7zX\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" /> Run in Google Colab</a>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vP9EENLd7zX"
      },
      "source": [
        "# ðŸ§ª Experiment 1: Warmup ðŸ”¥\n",
        "\n",
        "Hello, all, and welcome to your first hands-on experiment in the course! ðŸ¥³ If you're new to Co-Lab: welcome! Check out [this quick tutorial](https://www.youtube.com/watch?v=inN8seMm7UI&ab_channel=TensorFlow) to get situated.\n",
        "\n",
        "\n",
        "For this initial exercise, we'll warm up our coding skills and have a short introduction to some fundamental machine learning mechanics. For folks who are more advanced, we encourage you to leverage your background and skillsets to remix what we've started for you here! \n",
        "\n",
        "\n",
        "As a reminder, please share your results with your learning team on Slack! You can also share-out any highlights on the #share-out channel, too! \n",
        "\n",
        "Before we get started with the experiment, here are some **optional** readings that provide some additional context for the tools and techniques you'll explore:\n",
        "- Machine learning (ML) intro http://www.r2d3.us/visual-intro-to-machine-learning-part-1/ \n",
        "- AI and creativity: https://towardsdatascience.com/supercreativity-b4114ebd0357 \n",
        "- Generative models: https://openai.com/blog/generative-models/ \n",
        "- Neural network activations: https://distill.pub/2019/activation-atlas/ \n",
        "\n",
        "\n",
        "**Now, let's get to it!** In this experiment, we'll see three cool tricks with neural networks that show off their generative power:\n",
        "- Style Transfer ðŸ–¼ï¸\n",
        "- Deep Dream ðŸ’­\n",
        "- Neural Doodle âœï¸\n",
        "\n",
        "These are \"old\" tricks, but they can still teach us a whole lot about how neural networks work, and what they can be used for. Some key concepts to look out for are: **Loss function, Gradient (descent), Activation, Layer**.\n",
        "\n",
        "While the base code provides some examples how these tricks work, **you are encouraged to tweak parameters and see what results you get!**\n",
        "Hack around and change input images (we'll also guide you about where and how to do this, along the way); and remember to capture your creations! We can't wait to see what you come up with. \n",
        "\n",
        "Remember: All this code is free for you (and everyone) to use and share!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrdnkZOE3jyC"
      },
      "source": [
        "# There's some boilerplate code here below, run it (shift+Enter over each cell, or hit the 'play' button) ... Expand and take a peek if you'd like! We've provided additional context amongst the code."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfJ7g8Yy3jyG"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bw76lUN-3jyJ"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import functools\n",
        "import os\n",
        "import time\n",
        "\n",
        "import matplotlib.pylab as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from IPython.display import clear_output\n",
        "\n",
        "print(\"TF Version: \", tf.__version__)\n",
        "print(\"TF-Hub version: \", hub.__version__)\n",
        "print(\"Eager mode enabled: \", tf.executing_eagerly())\n",
        "print(\"GPU available: \", tf.test.is_gpu_available())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrL7MlbQ9KG5"
      },
      "source": [
        " To speed up the trainning process, please make sure that you are using **GPU** in the Google Colab:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0GhVzii9rJC"
      },
      "source": [
        "**To do this**. . .Go to menu : Runtime -> Change runtime type -> Hardware Accelerator -> GPU\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLk_Z6fZ9vAr"
      },
      "source": [
        "print(\"GPU available: \", tf.test.is_gpu_available())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFcF2ppq-CME"
      },
      "source": [
        "You should expect \"GPU available:  True\" in the previous output:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1hv6wZF3jyL"
      },
      "source": [
        "def load_image(image_url, image_size=(256, 256), preserve_aspect_ratio=True):\n",
        "    \"\"\"Loads and preprocesses images.\"\"\"\n",
        "    # Cache image file locally.\n",
        "    image_path = tf.keras.utils.get_file(os.path.basename(image_url)[-128:], image_url)\n",
        "    # Load and convert to float32 numpy array, add batch dimension, and normalize to range [0, 1].\n",
        "    img = plt.imread(image_path).astype(np.float32)[np.newaxis, ...] / 255.\n",
        "    if img.shape[-1] == 4:\n",
        "        img = img[..., :3] / tf.expand_dims(img[..., 3], -1) # pre multiply alpha\n",
        "        \n",
        "    img = tf.image.resize(img, image_size, preserve_aspect_ratio=True)\n",
        "    return img\n",
        "\n",
        "def show_n(images, titles=('',), w=8, colorbar=False):\n",
        "    n = len(images)\n",
        "    plt.figure(figsize=(w * n, w))\n",
        "    for i in range(n):\n",
        "        plt.subplot(1, n, 1 + i)\n",
        "        plt.imshow(images[i][0], aspect='equal')\n",
        "        plt.axis('off')\n",
        "        plt.title(titles[i] if len(titles) > i else '')\n",
        "        if colorbar:\n",
        "            plt.colorbar()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUCXKy8L3jyN"
      },
      "source": [
        "## Style Transfer ðŸ–¼ï¸\n",
        "\n",
        "**Style transfer** is a neat implementation that uses [deep convolutional networks](https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53) to tranfer artistic style from one image to another. \n",
        "\n",
        "Here's a reading that looks at this process a bit deeper: https://blog.paperspace.com/art-with-neural-networks/\n",
        "\n",
        "Also, if you're looking for some inspiration, check out: http://genekogan.com/works/style-transfer/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9zx0xEv3jyO"
      },
      "source": [
        "Let's start by loading some images from the net:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCFcEMBz3jyO"
      },
      "source": [
        "content_image_url = 'https://upload.wikimedia.org/wikipedia/commons/thumb/f/fd/Golden_Gate_Bridge_from_Battery_Spencer.jpg/640px-Golden_Gate_Bridge_from_Battery_Spencer.jpg'\n",
        "style_image_url = 'https://upload.wikimedia.org/wikipedia/commons/0/0a/The_Great_Wave_off_Kanagawa.jpg' \n",
        "output_image_size = 512 \n",
        "\n",
        "# The content image size can be arbitrary.\n",
        "# Note that you can load any images from the internet you like! Tinker with adding your own images, and seeing what results.\n",
        "# Also, see what happens with the final product when you switch the content image and the style image! \n",
        "content_img_size = (output_image_size, output_image_size)\n",
        "style_img_size = (256, 256)  # Recommended to keep it at 256.\n",
        "\n",
        "content_image = load_image(content_image_url, content_img_size)\n",
        "style_image = load_image(style_image_url, style_img_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1R4AfWaB3jyR"
      },
      "source": [
        "show_n([content_image, style_image], ['Content', 'Style'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_i5Pip43jyT"
      },
      "source": [
        "The goal is to apply the style of \"Style Image\" to \"Content Image\".\n",
        "\n",
        "Tensorflow Hub is a collection of models pre-trained and ready to use.\n",
        "We can find a style transfer model there too: https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5e94dJH3jyU"
      },
      "source": [
        "# Load image stylization module.\n",
        "\n",
        "hub_handle = 'https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/2'\n",
        "hub_module = hub.load(hub_handle)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9zBdRE23jyW"
      },
      "source": [
        "Using it is very straightforward:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5hCqJkX3jyX"
      },
      "source": [
        "# Stylize image.\n",
        "outputs = hub_module(content_image, style_image)\n",
        "stylized_image = outputs[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOIzLmmt3jyZ"
      },
      "source": [
        "And the reults are cool! Let's take a look at what we get: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4WV2h3e3jyZ"
      },
      "source": [
        "show_n([content_image, style_image, stylized_image], ['Content', 'Style', 'Stylized'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1GQbHfi3jyb"
      },
      "source": [
        "What's happening, here? Well, we iteratively change the \"Content Image\" to follow the \"Style Image\" on \"low visual layers.\" The deep convolutional neural network is divided into many layers (sometimes dozens and hundreds). Each layer successively learns more complex visual constructs. If we try to match just the lower layers - only the \"painting style\" will transfer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_Uw3qQM3jyc"
      },
      "source": [
        "Have a look at the list layers of the deep model below. You will see many of them belong to \"**[InceptionV3](https://cloud.google.com/tpu/docs/inception-v3-advanced)**\", which is a very well known deep model for classification. That part is known as the \"**backbone**\" - it's only used to exract visual features. The other layers show many convolution layers, resudial operators, and more."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBPt5uex3jyc"
      },
      "source": [
        "sorted([f.name for f in hub_module.variables])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CvK7Y9N3jye"
      },
      "source": [
        "## Deep Dream ðŸ’­\n",
        "\n",
        "**\"Deep Dream\"** refers to the process of amplifying the \"eye of the CNN.\" In other words, making the network \"hallucinate\" visual features (for example, eyes; faces) it learned from millions of images in plain images, where only hints of these structures exist.\n",
        "\n",
        "This is a good example of using Gradient Ascent. It's an inverse optimization problem, where instead of trying to minimize the loss, we try to maximize--or amplify--it. \n",
        "\n",
        "This tutorial is based on: https://www.tensorflow.org/beta/tutorials/generative/deepdream\n",
        "\n",
        "Let's see what the network finds in a Kandinsky:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCLM5kJA3jye"
      },
      "source": [
        "url = 'https://storage.googleapis.com/download.tensorflow.org/example_images/Vassily_Kandinsky%2C_1913_-_Composition_7.jpg'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8HeDCQw3jyh"
      },
      "source": [
        "content_image = load_image(url, image_size=(375,375))\n",
        "show_n([content_image])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxQD8RCP3jyj"
      },
      "source": [
        "Here we use the Inception network from Google. It was a winner of the ILSVRC circa 2013.\n",
        "\n",
        "Read about it here if interested in knowing more: https://medium.com/coinmonks/paper-review-of-googlenet-inception-v1-winner-of-ilsvlc-2014-image-classification-c2b3565a64e7"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YH-NF18_3jyk"
      },
      "source": [
        "base_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Zu3kzMk3jyo"
      },
      "source": [
        "We select a couple of layers from the network to maximize. These layers correspond to a size/level of visual construct. The lower layers will see very local visual features (lines, little blobs), while the higher ones will see e.g. eyes, faces, noses, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3qVcjEw3jyp"
      },
      "source": [
        "# Maximize the activations of these layers\n",
        "names = ['mixed3', 'mixed5']\n",
        "layers = [base_model.get_layer(name).output for name in names]\n",
        "\n",
        "# Create the feature extraction model\n",
        "dream_model = tf.keras.Model(inputs=base_model.input, outputs=layers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5AeDoJR3jyr"
      },
      "source": [
        "The Loss function guides our inverse optimization. Here it is very simple, it's just looking to increase the activation in our selected layers. You can explore the activations of InceptionNet here: https://distill.pub/2019/activation-atlas/ See every layer and neuron group \"like\" a different concept like: Pineapple, Hot dog, Jeans and Sleeping bag..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOIGTGum3jyr"
      },
      "source": [
        "def calc_loss(img, model):\n",
        "    # Pass forward the image through the model to retrieve the activations.\n",
        "    layer_activations = model(img)\n",
        "\n",
        "    losses = []\n",
        "    for act in layer_activations:\n",
        "        loss = tf.math.reduce_mean(act)\n",
        "        losses.append(loss)\n",
        "\n",
        "    return  tf.reduce_sum(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oz-K7EGm3jyt"
      },
      "source": [
        "Some code is obviously necessary to run the optimization... I'll spare you the details, but if you want to run this for youself you'd need to execute these cells."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-WRk5123jyu"
      },
      "source": [
        "@tf.function\n",
        "def deepdream(model, img, step_size):\n",
        "    print(\"deepdream\",type(img),img.shape)\n",
        "    with tf.GradientTape() as tape:\n",
        "        # This needs gradients relative to `img`\n",
        "        # `GradientTape` only watches `tf.Variable`s by default\n",
        "        tape.watch(img)\n",
        "        loss = calc_loss(img, model)\n",
        "\n",
        "    # Calculate the gradient of the loss with respect to the pixels of the input image.\n",
        "    gradients = tape.gradient(loss, img)\n",
        "\n",
        "    # Normalize the gradients.\n",
        "    gradients /= tf.math.reduce_std(gradients) + 1e-8 \n",
        "    \n",
        "    # In gradient ascent, the \"loss\" is maximized so that the input image increasingly \"excites\" the layers.\n",
        "    # You can update the image by directly adding the gradients (because they're the same shape!)\n",
        "    img = img + gradients*step_size\n",
        "    img = tf.clip_by_value(img, -1, 1)\n",
        "\n",
        "    return loss, img\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YNs66jM3jyw"
      },
      "source": [
        "def deprocess(img):\n",
        "    img = 127.5*(img + 1.0)\n",
        "    return tf.cast(img, tf.uint8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGWwy3o53jyy"
      },
      "source": [
        "def run_deep_dream_simple(model, img, steps=100, step_size=0.01):\n",
        "    # Convert from uint8 to the range expected by the model.\n",
        "    clear_output(wait=True)\n",
        "    show_n([deprocess(img)])\n",
        "\n",
        "    # main execution loop, every iteration applies little changes to the image\n",
        "    for step in range(steps):\n",
        "        loss, img = deepdream(model, img, step_size)\n",
        "\n",
        "        if step % 100 == 0:\n",
        "            clear_output(wait=True)\n",
        "            show_n([deprocess(img)])\n",
        "            print (\"Step {}, loss {}\".format(step, loss))\n",
        "\n",
        "    clear_output(wait=True)\n",
        "    show_n([deprocess(img)])\n",
        "\n",
        "    return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwymauDG3jy0"
      },
      "source": [
        "content_image_i3 = np.array(content_image) * 1.999 - 1.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GnUN6UH3jy2"
      },
      "source": [
        "dream_img = run_deep_dream_simple(model=dream_model, img=content_image_i3, steps=800, step_size=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUMxax3j3jy3"
      },
      "source": [
        "Above, we only get these little changes applied at a single \"scale.\" This means that the visual constructs (think \"eyes\") are of a single scale, for example 20x20 pixels. We would like to get \"eyes\" in any scale, even much bigger.\n",
        "\n",
        "The following loop will change the scale of the image itself (also konwn as an octave) and apply the same algorithm as above.\n",
        "\n",
        "The result is \"smoother,\" and is capable of finding bigger things. For example I can definitely see a \"bird\" and a few \"dogs\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lk4RAKvw3jy4"
      },
      "source": [
        "OCTAVE_SCALE = 1.3\n",
        "\n",
        "img = tf.constant(np.array(content_image_i3))\n",
        "base_shape = tf.cast(tf.shape(img)[1:-1], tf.float32)\n",
        "print(base_shape)\n",
        "\n",
        "for n in range(3):\n",
        "    new_shape = tf.cast(base_shape*(OCTAVE_SCALE**n), tf.int32)\n",
        "\n",
        "    img = tf.image.resize(img, new_shape).numpy()\n",
        "\n",
        "    img = run_deep_dream_simple(model=dream_model, img=img, steps=200, step_size=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axQjR6MQ3jy6"
      },
      "source": [
        "To get an even more hi-res result we utilize Tiling. We split the image to tiles and run the algorithm on each small tile at higher resolution.\n",
        "\n",
        "The end result is much higer resolution and shows the \"puppy slug\" effect you must have seen before.\n",
        "\n",
        "Again below are some extra code cells you'd need to run. Expand them out for details."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFWV03833jy6"
      },
      "source": [
        "def random_roll(img, maxroll):\n",
        "    # Randomly shift the image to avoid tiled boundaries.\n",
        "    shift = tf.random.uniform(shape=[2], minval=-maxroll, maxval=maxroll, dtype=tf.int32)\n",
        "    shift_down, shift_right = shift[0],shift[1] \n",
        "    img_rolled = tf.roll(tf.roll(img, shift_right, axis=1), shift_down, axis=0)\n",
        "    return shift_down, shift_right, img_rolled"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1ddKI2g3jy9"
      },
      "source": [
        "@tf.function\n",
        "def get_tiled_gradients(model, img, tile_size=512):\n",
        "    shift_down, shift_right, img_rolled = random_roll(img, tile_size)\n",
        "\n",
        "    # Initialize the image gradients to zero.\n",
        "    gradients = tf.zeros_like(img_rolled)\n",
        "\n",
        "    for x in tf.range(0, img_rolled.shape[0], tile_size):\n",
        "        for y in tf.range(0, img_rolled.shape[1], tile_size):\n",
        "            # Calculate the gradients for this tile.\n",
        "            with tf.GradientTape() as tape:\n",
        "                # This needs gradients relative to `img_rolled`.\n",
        "                # `GradientTape` only watches `tf.Variable`s by default.\n",
        "                tape.watch(img_rolled)\n",
        "\n",
        "                # Extract a tile out of the image.\n",
        "                img_tile = img_rolled[x:x+tile_size, y:y+tile_size]\n",
        "                loss = calc_loss(img_tile, model)\n",
        "\n",
        "                # Update the image gradients for this tile.\n",
        "                gradients = gradients + tape.gradient(loss, img_rolled)\n",
        "\n",
        "    # Undo the random shift applied to the image and its gradients.\n",
        "    gradients = tf.roll(tf.roll(gradients, -shift_right, axis=1), -shift_down, axis=0)\n",
        "\n",
        "    # Normalize the gradients.\n",
        "    gradients /= tf.math.reduce_std(gradients) + 1e-8 \n",
        "\n",
        "    return gradients "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4K2MSPNQ3jy_"
      },
      "source": [
        "def run_deep_dream_with_octaves(model, img, steps_per_octave=100, step_size=0.01, \n",
        "                                num_octaves=3, octave_scale=1.3):\n",
        "    for octave in range(num_octaves):\n",
        "        # Scale the image based on the octave\n",
        "        if octave>0:\n",
        "            new_size = tf.cast(img.shape[1:3], tf.float32)*octave_scale\n",
        "            img = tf.image.resize(img, tf.cast(new_size, tf.int32))\n",
        "\n",
        "        for step in range(steps_per_octave):\n",
        "            gradients = get_tiled_gradients(model, img)\n",
        "            img = img + gradients*step_size\n",
        "            img = tf.clip_by_value(img, -1, 1)\n",
        "\n",
        "            if step % 10 == 0:\n",
        "                clear_output(wait=True)\n",
        "                show_n([deprocess(img)])\n",
        "                print (\"Octave {}, Step {}\".format(octave, step))\n",
        "\n",
        "    clear_output(wait=True)\n",
        "    result = deprocess(img)\n",
        "    show_n([result])\n",
        "\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QX7WXplH3jzB"
      },
      "source": [
        "dream_img = run_deep_dream_with_octaves(model=dream_model, img=content_image_i3, step_size=0.01)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6K08gWq3jzC"
      },
      "source": [
        "## Neural Doodle âœï¸\n",
        "\n",
        "**Neural doodle** is effectively guided style transfer. First we mask the input style image with regions of different \"semantic\" value, (e.g., Grass, Sky, Rock, *etc.*). Then, given a new mask, we ask the model to complete in the details from the original image.\n",
        "\n",
        "There's going to be some significant code to make this happen. Feel free to explore within, and please do remix to make your own doodles! (Note that this code may take some time to run!). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkLR43_b3jzD"
      },
      "source": [
        "def gram_matrix(input_tensor):\n",
        "    result = tf.linalg.einsum('bijc,bijd->bcd', input_tensor, input_tensor)\n",
        "    input_shape = tf.shape(input_tensor)\n",
        "    num_locations = tf.cast(input_shape[1]*input_shape[2], tf.float32)\n",
        "    return result/(num_locations)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHWazYYC3jzE"
      },
      "source": [
        "Like we did before, we select some specific layers (this time from [VGG19](https://arxiv.org/abs/1409.1556)) to guide the transfer. See for \"Style\" we pick layers from multiple heights, so we get small scale features as well as larger ones:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfSaPced3jzE"
      },
      "source": [
        "content_layers = ['block5_conv2'] # content layer\n",
        "style_layers = ['block%d_conv1'%i for i in [1,2,3,4]] # Style layer of interest"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Vl7dVP03jzG"
      },
      "source": [
        "class StyleContentModel(tf.keras.models.Model):\n",
        "    def __init__(self, style_layers, content_layers, input_shape):\n",
        "        super(StyleContentModel, self).__init__()\n",
        "        self.content_layers = content_layers\n",
        "        self.style_layers = style_layers\n",
        "        self.num_style_layers = len(self.style_layers)\n",
        "        \n",
        "        # Load our model. Load pretrained VGG, trained on imagenet data\n",
        "        self.vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n",
        "        self.vgg.trainable = False\n",
        "\n",
        "        # mask model as a series of pooling\n",
        "        mask_input = tf.keras.layers.Input(shape=input_shape, name='mask_input')\n",
        "        x = mask_input\n",
        "        for layer in self.vgg.layers[1:]:\n",
        "            name = 'mask_%s' % layer.name\n",
        "            if 'conv' in layer.name:\n",
        "                x = tf.keras.layers.AveragePooling2D((3, 3), padding='same', strides=(1, 1), name=name)(x)\n",
        "            elif 'pool' in layer.name:\n",
        "                x = tf.keras.layers.AveragePooling2D((2, 2), name=name)(x)\n",
        "        self.mask_model = tf.keras.Model(mask_input, x)\n",
        "        self.vgg_layers_mask = tf.keras.Model([self.mask_model.input], \n",
        "                                              [self.mask_model.get_layer('mask_%s' % name).output for name in self.style_layers])\n",
        "\n",
        "        self.vgg_layers_style   = tf.keras.Model([self.vgg.input], \n",
        "                                                 [self.vgg.get_layer(name).output for name in self.style_layers])\n",
        "        self.vgg_layers_content = tf.keras.Model([self.vgg.input], \n",
        "                                                 [self.vgg.get_layer(name).output for name in self.content_layers])\n",
        "        \n",
        "\n",
        "    def call(self, inputs, masks, show_masks=False):\n",
        "        \"Expects float input in [0,1]\"\n",
        "        inputs = inputs * 255.0\n",
        "        style_outputs = []\n",
        "        mask_features = [self.vgg_layers_mask(tf.cast(tf.reshape(m, [1,*m.shape,1]), tf.float32)) for m in masks]\n",
        "        if show_masks:\n",
        "            show_n([tf.squeeze(m,-1) for m in mask_features[0]], colorbar=True)\n",
        "        \n",
        "        preprocessed_input = tf.keras.applications.vgg19.preprocess_input(inputs) \n",
        "        for m in mask_features:\n",
        "            outputs_style = self.vgg_layers_style(preprocessed_input)\n",
        "            outputs_style = [outputs_style[i] * m[i] for i in range(len(m))] # apply mask to image features\n",
        "            outputs_style = [gram_matrix(outputs_style[i]) / tf.reduce_mean(m[i]) for i in range(len(m))]\n",
        "            style_dict = {style_name:value for style_name, value in zip(self.style_layers, outputs_style)}\n",
        "            style_outputs += [style_dict]\n",
        "            \n",
        "        content_outputs = self.vgg_layers_content(preprocessed_input)\n",
        "        if not isinstance(content_outputs, list):\n",
        "            content_outputs = [content_outputs]\n",
        "        content_dict = {content_name:value for content_name, value in zip(self.content_layers, content_outputs)}\n",
        "\n",
        "        return {'content':content_dict, 'style':style_outputs}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lxA3bSF3jzI"
      },
      "source": [
        "def kmeans(xs, k):\n",
        "    assert xs.ndim == 2\n",
        "    try:\n",
        "        from sklearn.cluster import k_means\n",
        "        _, labels, _ = k_means(xs.astype('float64'), k)\n",
        "    except ImportError:\n",
        "        from scipy.cluster.vq import kmeans2\n",
        "        _, labels = kmeans2(xs, k, missing='raise')\n",
        "    return labels\n",
        "\n",
        "def load_mask_labels(style_mask_path, target_mask_path, num_labels, show_masks=False):\n",
        "    '''Load both target and style masks.\n",
        "    A mask image (nr x nc) with m labels/colors will be loaded as an array\n",
        "    '''\n",
        "    target_mask_img = load_image(target_mask_path)\n",
        "    style_mask_img = load_image(style_mask_path)\n",
        "    if show_masks:\n",
        "        show_n([style_mask_img, target_mask_img], ['Input mask', 'Target mask'])\n",
        "        \n",
        "    mask_vecs = np.vstack([tf.reshape(style_mask_img, (-1, 3)), tf.reshape(target_mask_img, (-1, 3))])\n",
        "\n",
        "    img_nrows, img_ncols = style_mask_img.shape[1:3]\n",
        "    labels = kmeans(mask_vecs, num_labels)\n",
        "    style_mask_label = labels[:img_nrows * img_ncols].reshape((img_nrows, img_ncols))\n",
        "    target_mask_label = labels[img_nrows * img_ncols:].reshape((img_nrows, img_ncols))\n",
        "\n",
        "    return ([style_mask_label == r for r in range(num_labels)], \n",
        "            [target_mask_label == r for r in range(num_labels)])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCde_QWd3jzJ"
      },
      "source": [
        "Consider this Max Liebermann painting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dN4WyCbS3jzJ"
      },
      "source": [
        "content_image = load_image(\"https://upload.wikimedia.org/wikipedia/commons/thumb/a/af/Max_Liebermann_Der_Nutzgarten_in_Wannsee_nach_Westen_1922.jpg/512px-Max_Liebermann_Der_Nutzgarten_in_Wannsee_nach_Westen_1922.jpg\")\n",
        "show_n([content_image],['Input'])\n",
        "#Once you run the Liebermann example, try adding your own image! "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjGvIjPI3jzL"
      },
      "source": [
        "We can create a masking of the painting with several regions. And then doodle another mask:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJ71daxn3jzL"
      },
      "source": [
        "style_masks, target_masks = load_mask_labels(\"https://github.com/mitmedialab/MAS.S60.Fall2020/raw/master/homework/source_mask.png\", \n",
        "                                             \"https://github.com/mitmedialab/MAS.S60.Fall2020/raw/master/homework/target_mask4.png\", \n",
        "                                             num_labels=4, show_masks=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pzk3J_kM3jzM"
      },
      "source": [
        "model = StyleContentModel(style_layers, content_layers, [*target_masks[0].shape, 1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7hghej_3jzO"
      },
      "source": [
        "This shows us the way the visual features are going to be masked according the regions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSPQCz9D3jzO"
      },
      "source": [
        "sources = model(content_image, style_masks, show_masks=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoN6m9ug3jzP"
      },
      "source": [
        "source_content, source_style = sources['content'], sources['style']\n",
        "\n",
        "num_style_layers = float(len(style_layers))\n",
        "num_content_layers = float(len(content_layers))\n",
        "\n",
        "style_weight=1.\n",
        "content_weight=0.1\n",
        "tv_weight=100."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMdEUh-73jzR"
      },
      "source": [
        "The Loss Function reveals that we are simply looking at the mean of squared distance (MSD) between our target and source image features:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNDCaTwg3jzR"
      },
      "source": [
        "def style_content_loss(outputs):\n",
        "    style_outputs = outputs['style']\n",
        "    content_outputs = outputs['content']\n",
        "    \n",
        "    loss = 0\n",
        "    for i in range(len(style_outputs)):\n",
        "        style_loss = tf.add_n([tf.reduce_mean((style_outputs[i][name]-source_style[i][name])**2) \n",
        "                               for name in style_outputs[i].keys()])\n",
        "        loss += style_loss * style_weight / num_style_layers\n",
        "\n",
        "    content_loss = tf.add_n([tf.reduce_mean((content_outputs[name]-source_content[name])**2) \n",
        "                     for name in content_outputs.keys()])\n",
        "    \n",
        "    return loss + content_loss * content_weight / num_content_layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vc3jq2KW3jzT"
      },
      "source": [
        "def clip_0_1(image):\n",
        "    return tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awWLVRsn3jzU"
      },
      "source": [
        "opt = tf.keras.optimizers.Adam(learning_rate=0.02, beta_1=0.99, epsilon=1e-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atUimH5K3jzW"
      },
      "source": [
        "@tf.function()\n",
        "def train_step(image, masks):\n",
        "    with tf.GradientTape() as tape:\n",
        "        outputs = model(image, masks)\n",
        "        loss = style_content_loss(outputs) + tv_weight * tf.image.total_variation(image)\n",
        "\n",
        "    grad = tape.gradient(loss, image)\n",
        "    opt.apply_gradients([(grad, image)])\n",
        "    image.assign(clip_0_1(image))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nABLrQVv3jzY"
      },
      "source": [
        "Below we set the input to the algorithm. \n",
        "\n",
        "Experiment with starting with a totally random image vs. starting from the original image. (Uncomment one line or the other)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhA7-OAy3jzY"
      },
      "source": [
        "# image = tf.Variable(tf.random.uniform([1, *target_masks[0].shape, 3]))\n",
        "image = tf.Variable(tf.identity(content_image))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYG5NAtk3jza"
      },
      "source": [
        "The execution loop simply applies the gradients imposed by the loss function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJxy6vRd3jza"
      },
      "source": [
        "start = time.time()\n",
        "\n",
        "epochs = 10\n",
        "steps_per_epoch = 100\n",
        "\n",
        "clear_output(wait=True)\n",
        "show_n([image.read_value()],[\"Train step: {}\".format(0)])\n",
        "\n",
        "step = 0\n",
        "for n in range(epochs):\n",
        "    for m in range(steps_per_epoch):\n",
        "        step += 1\n",
        "        train_step(image, target_masks)\n",
        "        print(\".\", end='')\n",
        "    clear_output(wait=True)\n",
        "    show_n([image.read_value()],[\"Train step: {}\".format(step)])\n",
        "\n",
        "end = time.time()\n",
        "print(\"Total time: {:.1f}\".format(end-start))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_QCr4WR3jzc"
      },
      "source": [
        "The results can be quite artistic! Although it's no Liebermann.. ðŸ˜€\n",
        "\n",
        "You should try your own doodles! Just change the doodle input image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTx0B8mXfFlV"
      },
      "source": [
        "**Challenge!** Once you've gone through the above steps, try to challenge yourself to complete the following prompts:\n",
        "- Can you change the season (weather, vegatation) in the the painting? For example, change spring into winter, using one of the tools we've used.\n",
        "- Can you find a way to increase the resolution of the images you've created today?"
      ]
    }
  ]
}
